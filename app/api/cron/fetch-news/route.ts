// Next.js: typ requestu i helper do zwracania JSON
import { NextRequest, NextResponse } from "next/server";

// RSS parser: bƒôdziemy parsowaƒá XML -> obiekty JS
import Parser from "rss-parser";

// Supabase client: zapisujemy newsy do bazy
import { createClient } from "@supabase/supabase-js";

// Wymuszamy Node runtime (wa≈ºne dla fetch + bibliotek)
export const runtime = "nodejs";

/**
 * 1) ≈ÅƒÖczymy siƒô z Supabase przez SERVICE ROLE
 *    - bo ten endpoint zapisuje do DB
 *    - dzia≈Ça tylko po stronie serwera
 */
const supabase = createClient(
  process.env.SUPABASE_URL!, // adres Supabase
  process.env.SUPABASE_SERVICE_ROLE_KEY! // klucz serwerowy (NIE publiczny)
);

/**
 * 2) RSS parser
 *    - UWAGA: bƒôdziemy parsowaƒá przez parseString(), a nie parseURL()
 *      bo chcemy mieƒá pe≈ÇnƒÖ kontrolƒô nad fetch (nag≈Ç√≥wki, status, content-type)
 */
const rssParser = new Parser();

// Mapujemy obce nazwy kategorii (np. z RSS) na slug-i w Supabase
const CATEGORY_ALIASES: Record<string, string> = {
  "recycling": "recykling",
  "circular-economy": "goz",
  "circular economy": "goz",
  "waste": "odpady", // je≈õli masz takƒÖ kategoriƒô
  "metals": "rynek-metali"
};

/**
 * 3) ≈πr√≥d≈Ça RSS
 *    - category_slug MUSI istnieƒá w tabeli news_categories (je≈õli masz FK)
 */
const RSS_SOURCES = [
  {
    name: "Recycling Portal",
    url: "https://recyclingportal.eu/feed/",
    category: "recykling",
    language: "de",
  },
  {
    name: "Waste Management World",
    url: "https://waste-management-world.com/rss.xml",
    category: "goz",
    language: "en",
  },
  {
    name: "Circular Online",
    url: "https://circularonline.co.uk/feed/",
    category: "goz",
    language: "en",
  },
];

export async function GET(req: NextRequest) {
  /**
   * 4) DEBUG MODE
   *    Je≈õli wejdziesz na:
   *    /api/cron/fetch-news?debug=1
   *    to dostaniesz dodatkowe dane diagnostyczne.
   */
  const debug = new URL(req.url).searchParams.get("debug") === "1";

  /**
   * 5) Zabezpieczenie endpointu:
   *    - Vercel Cron potrafi wysy≈Çaƒá nag≈Ç√≥wek x-vercel-cron: 1
   *    - rƒôcznie testujesz przez Authorization: Bearer CRON_SECRET
   */
  const isFromVercelCron = req.headers.get("x-vercel-cron") === "1";

  const authHeader = req.headers.get("authorization") || "";
  const expected = `Bearer ${process.env.CRON_SECRET || ""}`;

  const isAuthorizedBySecret =
    !!process.env.CRON_SECRET && authHeader.trim() === expected.trim();
let insertErrors = 0;
let lastInsertError: any = null;
  if (!isFromVercelCron && !isAuthorizedBySecret) {
    return NextResponse.json(
      {
        error: "Unauthorized",
        debug: {
          isFromVercelCron,
          hasAuthHeader: !!authHeader,
          authHeaderStartsWithBearer: authHeader
            .toLowerCase()
            .startsWith("bearer "),
          hasCronSecretEnv: !!process.env.CRON_SECRET,
        },
      },
      { status: 401 }
    );
  }

  /**
   * 6) Tu zbieramy wyniki (co zapisali≈õmy do DB)
   */
  const savedResults: Array<{ source: string; title: string }> = [];

  /**
   * 7) Tu zbieramy diagnostykƒô per feed (tylko je≈õli debug=1)
   */
  const debugFeeds: Array<any> = [];

  /**
   * 8) Iterujemy po ka≈ºdym ≈∫r√≥dle RSS
   */
  for (const source of RSS_SOURCES) {
    try {
      /**
       * 8.1) Pobieramy RSS przez fetch()
       *      Dodajemy User-Agent i Accept, bo niekt√≥re serwisy blokujƒÖ "anonimowe" boty.
       */
      const res = await fetch(source.url, {
        method: "GET",
        headers: {
          "User-Agent": "CIRCECbot/1.0 (+https://www.circec.eu)",
          Accept:
            "application/rss+xml, application/xml;q=0.9, text/xml;q=0.8, */*;q=0.7",
        },
        redirect: "follow",
      });

      const contentType = res.headers.get("content-type") || "";
      const xmlText = await res.text();

      // Je≈õli debug, zapisz podstawowe informacje o odpowiedzi HTTP
      if (debug) {
        debugFeeds.push({
          source: source.name,
          url: source.url,
          httpStatus: res.status,
          contentType,
          // pr√≥bka poczƒÖtku odpowiedzi - poka≈ºe, czy to XML czy HTML
          sample: xmlText.slice(0, 120),
        });
      }

      /**
       * 8.2) Je≈ºeli serwer nie zwr√≥ci≈Ç 200 OK, pomijamy ≈∫r√≥d≈Ço
       */
      if (!res.ok) {
        console.error(`RSS HTTP error (${source.name}):`, res.status);
        continue;
      }

      /**
       * 8.3) Parsujemy XML -> feed
       */
      const feed = await rssParser.parseString(xmlText);

      // Je≈ºeli feed nie ma items albo ma 0, to nic nie zapisujemy
      const items = feed.items || [];
      if (debug) {
        // dopisz informacjƒô, ile item√≥w parser znalaz≈Ç
        debugFeeds[debugFeeds.length - 1].parsedItemsCount = items.length;
      }

      /**
       * 8.4) Bierzemy max 5 news√≥w z danego ≈∫r√≥d≈Ça
       */
      for (const item of items.slice(0, 5)) {
        if (!item.title || !item.link) continue;

        /**
         * 8.5) Deduplikacja:
         *      je≈õli external_id (link) ju≈º istnieje w bazie, pomijamy
         */
        const { data: existing, error: existingError } = await supabase
          .from("news")
          .select("id")
          .eq("external_id", item.link)
          .maybeSingle();

        if (existingError) {
          console.error("B≈ÇƒÖd sprawdzania istnienia:", existingError);
          continue;
        }

        if (existing) continue;

        /**
         * 8.6) Generujemy slug:
         *      - zamieniamy znaki na "bezpieczny URL"
         *      - dok≈Çadamy Date.now() ≈ºeby unikaƒá kolizji
         */
        const baseSlug = item.title
          .toLowerCase()
          .replace(/[^a-z0-9ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+/g, "-")
          .replace(/(^-|-$)/g, "")
          .substring(0, 80);


       /**
 * Normalizujemy kategoriƒô:
 * - zamieniamy na ma≈Çe litery (≈ºeby "Recycling" i "recycling" dzia≈Ça≈Ço tak samo)
 * - sprawdzamy, czy jest alias
 * - je≈õli nie ma aliasu, u≈ºywamy tego co w source.category
 */
const categoryKey = source.category.trim().toLowerCase();
const categorySlug = CATEGORY_ALIASES[categoryKey] ?? source.category;

        /**
         * 8.7) Insert do DB:
         *      status = 'review' ‚Üí admin p√≥≈∫niej publikuje
         */
        const { error } = await supabase.from("news").insert({
          title: item.title,
          slug: `${baseSlug}-${Date.now()}`,
          excerpt: item.contentSnippet?.substring(0, 300) || "",
          content: (item as any).content || "",
          source_type: "rss",
          source_name: source.name,
          source_url: item.link,
          external_id: item.link,
          category_slug: categorySlug, // lub source.category je≈õli jeszcze bez alias√≥w
          status: "review",
          published_at: item.pubDate ? new Date(item.pubDate) : new Date(),
        });


        if (error) {
          insertErrors++;
          lastInsertError = {
            message: error.message,
            details: (error as any).details,
            hint: (error as any).hint,
            code: (error as any).code,
          };
          console.error("B≈ÇƒÖd zapisu do Supabase:", error);
        } else {
          savedResults.push({
            source: source.name,
            title: item.title.substring(0, 60),
          });
        }

        
      }
    } catch (err) {
      console.error(`B≈ÇƒÖd RSS (${source.name}):`, err);
    }
  }

  /**
   * 9) Zwracamy wynik:
   *    - fetched: ile zapisali≈õmy
   *    - items: kr√≥tkie info o zapisanych
   *    - debugFeeds: diagnostyka (tylko gdy debug=1)
   */
  return NextResponse.json({
    success: true,
    fetched: savedResults.length,
    items: savedResults,
    debug: debug
  ? {
      debugFeedsCount: debugFeeds.length,
      debugFeeds,

      // üëá DODAJEMY DIAGNOSTYKƒò INSERTU
      insertErrors,       // ile insert√≥w siƒô nie uda≈Ço
      lastInsertError,    // szczeg√≥≈Çy ostatniego b≈Çƒôdu (je≈õli by≈Ç)
    }
  : undefined,
    version: "fetch-news-2026-02-18-1"
  });
}
